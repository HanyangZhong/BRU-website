This project investigates how large language models (LLMs) handle **cognitive biases** during multiple-choice question answering (MCQA), challenging the prevailing assumption that all biases are harmful. Instead, we explore how certain *rational deviations*—biases that mimic human heuristics—can sometimes enhance decision-making efficiency.

We introduce **heuristic moderation** and a novel **abstention mechanism**, enabling LLMs to refrain from answering when uncertain. This approach significantly reduces error rates and improves answer validity. Our experiments, conducted on the newly developed **BRU (Balance Rigor and Utility)** dataset, show that selective bias inspection—especially *Specific Bias Inspection (SBI)*—combined with abstention leads to substantial performance gains in models like GPT-4 and Gemini 1.0 Pro.

🔍 **Key Features**:
- A principled framework for understanding and managing rational deviations in LLMs.
- Strategic use of abstention to reduce incorrect answers in uncertain scenarios.
- The BRU dataset: 200+ expertly designed MCQs targeting 8 types of cognitive bias (e.g., base rate fallacy, sunk cost, conjunction fallacy).
- New evaluation metrics: **Decisiveness Rate**, **Error Rate**, and **Valid Vote Accuracy**.
- A **Bias Detection Module** (powered by GPT-4o) that dynamically adjusts inspection scope via feedback loops.

📊 **Highlights**:
- 🧠 GPT-4 + SBI + Abstention achieves **93.5% Valid Vote Accuracy**, with only **3.9% Error Rate**.
- 💡 SBI consistently outperforms General Bias Inspection (GBI), showing the value of targeted prompting.
- 🚫 LLMs exhibit fewer fallacies when given the option to *say "I don't know"*.

🔗 Full paper, dataset, and demo available at: [hanyangzhong.github.io/BRU-website](https://hanyangzhong.github.io/BRU-website)

---

> Cognitive biases aren't always bugs — sometimes, they're shortcuts. Our work shows how LLMs can balance **human-like heuristics** with **computational precision**, improving both **fairness** and **utility** in decision-making.
