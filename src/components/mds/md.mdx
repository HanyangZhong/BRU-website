# Introduction
This project investigates how large language models (LLMs) handle **cognitive biases** during multiple-choice question answering (MCQA), challenging the prevailing assumption that all biases are harmful. Instead, we explore how certain *rational deviations*‚Äîbiases that mimic human heuristics‚Äîcan sometimes enhance decision-making efficiency.

We introduce **heuristic moderation** and a novel **abstention mechanism**, enabling LLMs to refrain from answering when uncertain. This approach significantly reduces error rates and improves answer validity. Our experiments, conducted on the newly developed **BRU (Balance Rigor and Utility)** dataset, show that selective bias inspection‚Äîespecially *Specific Bias Inspection (SBI)*‚Äîcombined with abstention leads to substantial performance gains in models like GPT-4 and Gemini 1.0 Pro.

üîç **Key Features**:
- A principled framework for understanding and managing rational deviations in LLMs.
- Strategic use of abstention to reduce incorrect answers in uncertain scenarios.
- The BRU dataset: 200+ expertly designed MCQs targeting 8 types of cognitive bias (e.g., base rate fallacy, sunk cost, conjunction fallacy).
- New evaluation metrics: **Decisiveness Rate**, **Error Rate**, and **Valid Vote Accuracy**.
- A **Bias Detection Module** (powered by GPT-4o) that dynamically adjusts inspection scope via feedback loops.

üìä **Highlights**:
- üß† GPT-4 + SBI + Abstention achieves **93.5% Valid Vote Accuracy**, with only **3.9% Error Rate**.
- üí° SBI consistently outperforms General Bias Inspection (GBI), showing the value of targeted prompting.
- üö´ LLMs exhibit fewer fallacies when given the option to *say "I don't know"*.

# Our Workflow

![](./carousel/figure02_new.png)
QA examples from GPT-4. The Conjunction Fallacy is a subset of cognitive biases. Scaling the scope of bias inspection can influence rational deviations, thereby impacting the outcomes of LLMs' reasoning. To address this, we propose a feedback loop Bias Detection module to identify the type of bias and adjust the inspection scope when an abstention from answering is considered. This approach ensures that LLMs provide more accurate responses by systematically addressing biases during decision-making.
# Dataset information

![](./carousel/frames.png)
This diagram pertains to the specific details of dataset design and the classification of questions, with the numbers in parentheses indicating the quantity of questions in each category.
# Numerical results

![Figure Title](./carousel/new_first2.png)
Valid vote accuracy and error rates on the BRU dataset for LLMs balancing rational deviations, both with and without the option to abstain.

![Figure Title](./carousel/new_figure4.png)
Prediction accuracy and error rate of GPT-4, Gemini 1.0 Pro, and LLaMA3-70B in Non-Abstention and Abstention experiments (%) on the BRU dataset with different prompting strategies. Bold numbers indicate the relative extrema. Differences between Standard groups with and without abstention are shown with +- values in black.

![Figure Title](./carousel/new_figure5.png)
The combination of TT, TF, FT, FF, and O rates for GPT-4, Gemini 1.0 Pro, and LLaMA3-70B on the BRU dataset using different prompting strategies. 'NA-' denotes Non-Abstention, 'A-' denotes Abstention, and 'Sta' represents the Standard used for comparison.

# Numerical results details
## GBI and SBI
![Figure Title](./carousel/table3.png)
Model GPT-4, Gemini 1.0 Pro and LLaMA3-70B prediction accuracy for Non-Abstention experiments (%) on BRU dataset
![Figure Title](./carousel/table4.png)
Model GPT-4, Gemini 1.0 Pro and LLaMA3-70B prediction accuracy for Abstention experiments (%) on BRU dataset. ‚ÄôN/A‚Äô indicates that there was no response in this category.
![Figure Title](./carousel/table5.png)
The percentage (%) of TT, TF, FT, FF, and O in abstention experiment for GPT-4, Gemini 1.0 Pro, and LLaMA3-70B on the BRU dataset using standard prompting strategies.
![Figure Title](./carousel/table6.png)
The percentage (%) of TT, TF, FT, FF, and O in abstention experiment for GPT-4, Gemini 1.0 Pro, and LLaMA3-70B on the BRU dataset using GBI prompting strategies.
![Figure Title](./carousel/table7.png)
The percentage (%) of TT, TF, FT, FF, and O in abstention experiment for GPT-4, Gemini 1.0 Pro, and LLaMA3-70B on the BRU dataset using SBI prompting strategies.

## Transformation results details
![Figure Title](./carousel/table8.png)
This table presents the accuracy (%) of GPT-4o in detecting specific bias traps within the Bias Detection Loop.
‚ÄùDirect matching‚Äù refers to instances where GPT-4o accurately identifies the exact subtype associated with a particular bias. In
contrast, ‚Äùindirect matching‚Äù denotes cases where GPT-4o recognizes either the broader parent category or a synonym of the
bias subtype.


![Figure Title](./carousel/table9.png)

![Figure Title](./carousel/table8.png)

![Figure Title](./carousel/table7.png)

![Figure Title](./carousel/table8.png)