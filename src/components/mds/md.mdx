# Introduction
This project investigates how large language models (LLMs) handle **cognitive biases** during multiple-choice question answering (MCQA), challenging the prevailing assumption that all biases are harmful. Instead, we explore how certain *rational deviations*‚Äîbiases that mimic human heuristics‚Äîcan sometimes enhance decision-making efficiency.

We introduce **heuristic moderation** and a novel **abstention mechanism**, enabling LLMs to refrain from answering when uncertain. This approach significantly reduces error rates and improves answer validity. Our experiments, conducted on the newly developed **BRU (Balance Rigor and Utility)** dataset, show that selective bias inspection‚Äîespecially *Specific Bias Inspection (SBI)*‚Äîcombined with abstention leads to substantial performance gains in models like GPT-4 and Gemini 1.0 Pro.

üîç **Key Features**:
- A principled framework for understanding and managing rational deviations in LLMs.
- Strategic use of abstention to reduce incorrect answers in uncertain scenarios.
- The BRU dataset: 200+ expertly designed MCQs targeting 8 types of cognitive bias (e.g., base rate fallacy, sunk cost, conjunction fallacy).
- New evaluation metrics: **Decisiveness Rate**, **Error Rate**, and **Valid Vote Accuracy**.
- A **Bias Detection Module** (powered by GPT-4o) that dynamically adjusts inspection scope via feedback loops.

üìä **Highlights**:
- üß† GPT-4 + SBI + Abstention achieves **93.5% Valid Vote Accuracy**, with only **3.9% Error Rate**.
- üí° SBI consistently outperforms General Bias Inspection (GBI), showing the value of targeted prompting.
- üö´ LLMs exhibit fewer fallacies when given the option to *say "I don't know"*.

# Our Workflow

![](./carousel/figure02_new.png)
QA examples from GPT-4. The Conjunction Fallacy is a subset of cognitive biases. Scaling the scope of bias inspection can influence rational deviations, thereby impacting the outcomes of LLMs' reasoning. To address this, we propose a feedback loop Bias Detection module to identify the type of bias and adjust the inspection scope when an abstention from answering is considered. This approach ensures that LLMs provide more accurate responses by systematically addressing biases during decision-making.
# Dataset information

![](./carousel/frames.png)
This diagram pertains to the specific details of dataset design and the classification of questions, with the numbers in parentheses indicating the quantity of questions in each category.
# Numerical results

![Figure Title](./carousel/new_first2.png)
Valid vote accuracy and error rates on the BRU dataset for LLMs balancing rational deviations, both with and without the option to abstain.

![Figure Title](./carousel/new_figure4.png)
Prediction accuracy and error rate of GPT-4, Gemini 1.0 Pro, and LLaMA3-70B in Non-Abstention and Abstention experiments (%) on the BRU dataset with different prompting strategies. Bold numbers indicate the relative extrema. Differences between Standard groups with and without abstention are shown with +- values in black.

![Figure Title](./carousel/new_figure5.png)
The combination of TT, TF, FT, FF, and O rates for GPT-4, Gemini 1.0 Pro, and LLaMA3-70B on the BRU dataset using different prompting strategies. 'NA-' denotes Non-Abstention, 'A-' denotes Abstention, and 'Sta' represents the Standard used for comparison.

