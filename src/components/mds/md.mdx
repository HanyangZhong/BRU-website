This project investigates how large language models (LLMs) handle **cognitive biases** during multiple-choice question answering (MCQA), challenging the prevailing assumption that all biases are harmful. Instead, we explore how certain *rational deviations*‚Äîbiases that mimic human heuristics‚Äîcan sometimes enhance decision-making efficiency.

We introduce **heuristic moderation** and a novel **abstention mechanism**, enabling LLMs to refrain from answering when uncertain. This approach significantly reduces error rates and improves answer validity. Our experiments, conducted on the newly developed **BRU (Balance Rigor and Utility)** dataset, show that selective bias inspection‚Äîespecially *Specific Bias Inspection (SBI)*‚Äîcombined with abstention leads to substantial performance gains in models like GPT-4 and Gemini 1.0 Pro.

üîç **Key Features**:
- A principled framework for understanding and managing rational deviations in LLMs.
- Strategic use of abstention to reduce incorrect answers in uncertain scenarios.
- The BRU dataset: 200+ expertly designed MCQs targeting 8 types of cognitive bias (e.g., base rate fallacy, sunk cost, conjunction fallacy).
- New evaluation metrics: **Decisiveness Rate**, **Error Rate**, and **Valid Vote Accuracy**.
- A **Bias Detection Module** (powered by GPT-4o) that dynamically adjusts inspection scope via feedback loops.

üìä **Highlights**:
- üß† GPT-4 + SBI + Abstention achieves **93.5% Valid Vote Accuracy**, with only **3.9% Error Rate**.
- üí° SBI consistently outperforms General Bias Inspection (GBI), showing the value of targeted prompting.
- üö´ LLMs exhibit fewer fallacies when given the option to *say "I don't know"*.

---

![QA examples from GPT-4. The Conjunction Fallacy is a subset of cognitive biases. Scaling the scope of bias inspection can influence rational deviations, thereby impacting the outcomes of LLMs' reasoning. To address this, we propose a feedback loop Bias Detection module to identify the type of bias and adjust the inspection scope when an abstention from answering is considered. This approach ensures that LLMs provide more accurate responses by systematically addressing biases during decision-making.](./public/carousel/figure02_new.png)


![Figure Title](./public/carousel/frames.png)


![Figure Title](./public/carousel/new_first2.png)


![Figure Title](./public/carousel/new_figure4.png)

![Figure Title](./public/carousel/new_figure5.png)
